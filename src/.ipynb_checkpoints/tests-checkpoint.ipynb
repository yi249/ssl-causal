{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[119], line 22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpgm\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpgm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdscm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DSCM\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtrainer\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01margparse\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/causal-gen/src/pgm/dscm.py:7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dict\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TraceStorage_ELBO\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tensor, nn\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils_pgm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_nan\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'layers'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import struct\n",
    "from typing import Dict, List, Optional, Tuple, TypedDict\n",
    "from hps import add_arguments, setup_hparams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as TF\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hps import Hparams\n",
    "from utils import log_standardize, normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import pgm\n",
    "import trainer\n",
    "import argparse\n",
    "from vae import HVAE\n",
    "from main import main"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T15:09:36.439688642Z",
     "start_time": "2023-11-08T15:09:36.385651042Z"
    }
   },
   "id": "df8ed71874881207"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-07T18:02:08.649700774Z",
     "start_time": "2023-11-07T18:02:08.646143241Z"
    }
   },
   "outputs": [],
   "source": [
    "def _load_uint8(f):\n",
    "    idx_dtype, ndim = struct.unpack(\"BBBB\", f.read(4))[2:]\n",
    "    shape = struct.unpack(\">\" + \"I\" * ndim, f.read(4 * ndim))\n",
    "    buffer_length = int(np.prod(shape))\n",
    "    data = np.frombuffer(f.read(buffer_length), dtype=np.uint8).reshape(shape)\n",
    "    return data\n",
    "\n",
    "def load_idx(path: str) -> np.ndarray:\n",
    "    \"\"\"Reads an array in IDX format from disk.\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the input file. Will uncompress with `gzip` if path ends in '.gz'.\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Output array of dtype ``uint8``.\n",
    "    References\n",
    "    ----------\n",
    "    http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    open_fcn = gzip.open if path.endswith(\".gz\") else open\n",
    "    with open_fcn(path, \"rb\") as f:\n",
    "        return _load_uint8(f)\n",
    "\n",
    "\n",
    "def _get_paths(root_dir, train):\n",
    "    prefix = \"train\" if train else \"t10k\"\n",
    "    images_filename = prefix + \"-images-idx3-ubyte.gz\"\n",
    "    labels_filename = prefix + \"-labels-idx1-ubyte.gz\"\n",
    "    metrics_filename = prefix + \"-morpho.csv\"\n",
    "    images_path = os.path.join(root_dir, images_filename)\n",
    "    labels_path = os.path.join(root_dir, labels_filename)\n",
    "    metrics_path = os.path.join(root_dir, metrics_filename)\n",
    "    return images_path, labels_path, metrics_path\n",
    "def load_morphomnist_like(\n",
    "    root_dir, train: bool = True, columns=None\n",
    ") -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        root_dir: path to data directory\n",
    "        train: whether to load the training subset (``True``, ``'train-*'`` files) or the test\n",
    "            subset (``False``, ``'t10k-*'`` files)\n",
    "        columns: list of morphometrics to load; by default (``None``) loads the image index and\n",
    "            all available metrics: area, length, thickness, slant, width, and height\n",
    "    Returns:\n",
    "        images, labels, metrics\n",
    "    \"\"\"\n",
    "    images_path, labels_path, metrics_path = _get_paths(root_dir, train)\n",
    "    images = load_idx(images_path)\n",
    "    labels = load_idx(labels_path)\n",
    "\n",
    "    if columns is not None and \"index\" not in columns:\n",
    "        usecols = [\"index\"] + list(columns)\n",
    "    else:\n",
    "        usecols = columns\n",
    "    metrics = pd.read_csv(metrics_path, usecols=usecols, index_col=\"index\")\n",
    "    return images, labels, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data = load_morphomnist_like('/home/yasin/Desktop/causal-gen/datasets/morphomnist')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T18:03:08.457519305Z",
     "start_time": "2023-11-07T18:03:08.316705807Z"
    }
   },
   "id": "e593926e9a109dd7"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "class MorphoMNIST(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        train: bool = True,\n",
    "        transform: Optional[torchvision.transforms.Compose] = None,\n",
    "        columns: Optional[List[str]] = None,\n",
    "        norm: Optional[str] = None,\n",
    "        concat_pa: bool = True,\n",
    "    ):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.columns = columns\n",
    "        self.concat_pa = concat_pa\n",
    "        self.norm = norm\n",
    "\n",
    "        cols_not_digit = [c for c in self.columns if c != \"digit\"]\n",
    "        images, labels, metrics_df = load_morphomnist_like(\n",
    "            root_dir, train, cols_not_digit\n",
    "        )\n",
    "        self.images = torch.from_numpy(np.array(images)).unsqueeze(1)\n",
    "        self.labels = F.one_hot(\n",
    "            torch.from_numpy(np.array(labels)).long(), num_classes=10\n",
    "        )\n",
    "\n",
    "        if self.columns is None:\n",
    "            self.columns = metrics_df.columns\n",
    "        self.samples = {k: torch.tensor(metrics_df[k]) for k in cols_not_digit}\n",
    "\n",
    "        self.min_max = {\n",
    "            \"thickness\": [0.87598526, 6.255515],\n",
    "            \"intensity\": [66.601204, 254.90317],\n",
    "        }\n",
    "\n",
    "        for k, v in self.samples.items():  # optional preprocessing\n",
    "            print(f\"{k} normalization: {norm}\")\n",
    "            if norm == \"[-1,1]\":\n",
    "                self.samples[k] = normalize(\n",
    "                    v, x_min=self.min_max[k][0], x_max=self.min_max[k][1]\n",
    "                )\n",
    "            elif norm == \"[0,1]\":\n",
    "                self.samples[k] = normalize(\n",
    "                    v, x_min=self.min_max[k][0], x_max=self.min_max[k][1], zero_one=True\n",
    "                )\n",
    "            elif norm == None:\n",
    "                pass\n",
    "            else:\n",
    "                NotImplementedError(f\"{norm} not implemented.\")\n",
    "        print(f\"#samples: {len(metrics_df)}\\n\")\n",
    "\n",
    "        self.samples.update({\"digit\": self.labels})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Tensor]:\n",
    "        sample = {}\n",
    "        sample[\"x\"] = self.images[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            sample[\"x\"] = self.transform(sample[\"x\"])\n",
    "\n",
    "        if self.concat_pa:\n",
    "            sample[\"pa\"] = torch.cat(\n",
    "                [\n",
    "                    v[idx] if k == \"digit\" else torch.tensor([v[idx]])\n",
    "                    for k, v in self.samples.items()\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            sample.update({k: v[idx] for k, v in self.samples.items()})\n",
    "        return sample\n",
    "\n",
    "\n",
    "def morphomnist():\n",
    "    # Load data\n",
    "\n",
    "    data_dir = \"/home/yasin/Desktop/causal-gen/datasets/morphomnist\"\n",
    "\n",
    "    augmentation = {\n",
    "        \"train\": TF.Compose(\n",
    "            [\n",
    "                TF.RandomCrop((32, 32), padding=4),\n",
    "            ]\n",
    "        ),\n",
    "        \"eval\": TF.Compose(\n",
    "            [\n",
    "                TF.Pad(padding=2),  # (32, 32)\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    datasets = {}\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        datasets[split] = MorphoMNIST(\n",
    "            root_dir=data_dir,\n",
    "            train=(split == \"train\"),  # test set is valid set\n",
    "            transform=augmentation[(\"eval\" if split != \"train\" else split)],\n",
    "            columns=[\"thickness\", \"intensity\", \"digit\"],\n",
    "            norm=\"[-1,1]\",\n",
    "            concat_pa=True,\n",
    "        )\n",
    "    return datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T14:11:56.785726370Z",
     "start_time": "2023-11-08T14:11:56.770584433Z"
    }
   },
   "id": "5bf1a7b6ae3db75c"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "def setup_dataloaders(bs):\n",
    "    datasets = morphomnist()\n",
    "    kwargs = {\n",
    "        \"batch_size\": bs,\n",
    "        \"num_workers\": 4,\n",
    "        \"pin_memory\": True\n",
    "    }\n",
    "    dataloaders = {}\n",
    "    dataloaders[\"train\"] = DataLoader(datasets[\"train\"], shuffle=True, drop_last=True, **kwargs)\n",
    "    dataloaders[\"valid\"] = DataLoader(datasets[\"valid\"], shuffle=False, **kwargs)\n",
    "    dataloaders[\"test\"] = DataLoader(datasets[\"test\"], shuffle=False, **kwargs)\n",
    "    return dataloaders"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T14:11:57.150392403Z",
     "start_time": "2023-11-08T14:11:57.132459115Z"
    }
   },
   "id": "b8e6fb9087905022"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thickness normalization: [-1,1]\n",
      "max: 6.255515, min: 0.87598526\n",
      "intensity normalization: [-1,1]\n",
      "max: 254.90317, min: 66.601204\n",
      "#samples: 60000\n",
      "\n",
      "thickness normalization: [-1,1]\n",
      "max: 6.255515, min: 0.87598526\n",
      "intensity normalization: [-1,1]\n",
      "max: 254.90317, min: 66.601204\n",
      "#samples: 10000\n",
      "\n",
      "thickness normalization: [-1,1]\n",
      "max: 6.255515, min: 0.87598526\n",
      "intensity normalization: [-1,1]\n",
      "max: 254.90317, min: 66.601204\n",
      "#samples: 10000\n"
     ]
    }
   ],
   "source": [
    "dataloader = setup_dataloaders(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T14:11:58.366350973Z",
     "start_time": "2023-11-08T14:11:57.675903068Z"
    }
   },
   "id": "170bfe0012b87e06"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "loader = enumerate(dataloader['train'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T14:11:58.899249238Z",
     "start_time": "2023-11-08T14:11:58.802485331Z"
    }
   },
   "id": "8d91bb3b1c070472"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4486,  0.2572,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  1.0000,  0.0000,  0.0000]], dtype=torch.float64)\n",
      "torch.Size([1, 12, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in loader:\n",
    "    print(batch['pa'])\n",
    "    batch[\"pa\"] = batch[\"pa\"][..., None, None]#.repeat(1, 1, *(32,) * 2)\n",
    "    print(batch['pa'].size())\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T14:19:03.784125622Z",
     "start_time": "2023-11-08T14:19:03.771664600Z"
    }
   },
   "id": "be0d349acef04469"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
